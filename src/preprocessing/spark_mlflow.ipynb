{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "from io import BytesIO\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_dataset(url, extract_path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with tarfile.open(fileobj=BytesIO(response.content), mode=\"r:gz\") as tar:\n",
    "            tar.extractall(path=extract_path)\n",
    "    else:\n",
    "        print(\"Failed to download the dataset. Please check the URL or your internet connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reviews(spark, path, label):\n",
    "    df = spark.read.text(os.path.join(path, \"*.txt\"))\n",
    "    df = df.withColumn(\"label\", lit(label))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_df, max_words=10000, max_len=100):\n",
    "    # Tokenization and Stopwords Removal using PySpark\n",
    "    tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"words\")\n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "    pipeline = Pipeline(stages=[tokenizer, remover])\n",
    "    train_df = pipeline.fit(train_df).transform(train_df)\n",
    "    \n",
    "    # Convert filtered words to list\n",
    "    filtered_words_list = train_df.select(\"filtered_words\").rdd.flatMap(lambda x: x).collect()\n",
    "    filtered_words_str = [\" \".join(words) for words in filtered_words_list]\n",
    "    \n",
    "    # Tokenization using Keras Tokenizer\n",
    "    keras_tokenizer = KerasTokenizer(num_words=max_words)\n",
    "    keras_tokenizer.fit_on_texts(filtered_words_str)\n",
    "    sequences = keras_tokenizer.texts_to_sequences(filtered_words_str)\n",
    "\n",
    "    X = pad_sequences(sequences, maxlen=max_len)\n",
    "    y = np.array(train_df.select(\"label\").collect())\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_configurations = [\n",
    "    {'max_words': 10000, 'max_len': 100, 'embedding_dim': 128, 'lstm_units': 128, 'dropout': 0.2},\n",
    "    {'max_words': 10000, 'max_len': 100, 'embedding_dim': 128, 'lstm_units': 256, 'dropout': 0.2},\n",
    "    {'max_words': 10000, 'max_len': 100, 'embedding_dim': 128, 'lstm_units': 128, 'dropout': 0.1},\n",
    "    {'max_words': 10000, 'max_len': 100, 'embedding_dim': 256, 'lstm_units': 128, 'dropout': 0.2},\n",
    "    {'max_words': 10000, 'max_len': 150, 'embedding_dim': 128, 'lstm_units': 128, 'dropout': 0.2},\n",
    "    {'max_words': 5000, 'max_len': 100, 'embedding_dim': 128, 'lstm_units': 128, 'dropout': 0.2},\n",
    "    {'max_words': 10000, 'max_len': 100, 'embedding_dim': 128, 'lstm_units': 128, 'dropout': 0.2, 'activation': 'relu'},\n",
    "    {'max_words': 10000, 'max_len': 100, 'embedding_dim': 128, 'lstm_units': 128, 'dropout': 0.2, 'optimizer': 'rmsprop'}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(params,x_train,x_val,y_train,y_val):\n",
    "    mlflow.autolog()\n",
    "\n",
    "    max_words = params['max_words']\n",
    "    max_len = params['max_len']\n",
    "    embedding_dim = params['embedding_dim']\n",
    "    lstm_units = params['lstm_units']\n",
    "    dropout = params['dropout']\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim))\n",
    "    model.add(SpatialDropout1D(dropout))\n",
    "    model.add(LSTM(lstm_units, dropout=dropout, recurrent_dropout=dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=2)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',AUC()])\n",
    "\n",
    "    model.fit(x_train,y_train, epochs=2, validation_data=(x_val, y_val), batch_size = 64, callbacks=[es])\n",
    "\n",
    "    eval_result = model.evaluate(x_val,y_val,batch_size=64)\n",
    "\n",
    "    mlflow.end_run()\n",
    "\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logistic_params = [\n",
    "    {'penalty': 'l2', 'C': 0.01, 'solver': 'lbfgs', 'max_iter': 100},\n",
    "    {'penalty': 'l2', 'C': 0.1, 'solver': 'lbfgs', 'max_iter': 200},\n",
    "    {'penalty': 'l2', 'C': 1.0, 'solver': 'lbfgs', 'max_iter': 300},\n",
    "    {'penalty': 'l1', 'C': 0.01, 'solver': 'saga', 'max_iter': 100},\n",
    "    {'penalty': 'l1', 'C': 0.1, 'solver': 'saga', 'max_iter': 200},\n",
    "    {'penalty': 'l1', 'C': 1.0, 'solver': 'saga', 'max_iter': 300},\n",
    "    {'penalty': 'elasticnet', 'C': 0.01, 'solver': 'saga', 'max_iter': 100, 'l1_ratio': 0.5},\n",
    "    {'penalty': 'elasticnet', 'C': 0.1, 'solver': 'saga', 'max_iter': 200, 'l1_ratio': 0.5},\n",
    "    {'penalty': 'elasticnet', 'C': 1.0, 'solver': 'saga', 'max_iter': 300, 'l1_ratio': 0.5},\n",
    "    {'penalty': 'none', 'C': 1.0, 'solver': 'newton-cg', 'max_iter': 100},\n",
    "    {'penalty': 'none', 'C': 1.0, 'solver': 'newton-cg', 'max_iter': 200},\n",
    "    {'penalty': 'none', 'C': 1.0, 'solver': 'newton-cg', 'max_iter': 300}\n",
    "]\n",
    "\n",
    "\n",
    "def train_and_log_models(params_list, X, y):\n",
    "    # Setting the MLflow experiment\n",
    "    mlflow.set_experiment(\"Logistic_Regression_Experiments\")\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    for params in params_list:\n",
    "        with mlflow.start_run():\n",
    "            # Create a logistic regression model with the specified parameters\n",
    "            model = LogisticRegression(**params)\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict on the validation set\n",
    "            y_pred = model.predict(X_val)\n",
    "            y_proba = model.predict_proba(X_val)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_val, y_pred)\n",
    "            roc_auc = roc_auc_score(y_val, y_proba)\n",
    "            f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "            # Log parameters and metrics to MLflow\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "            # Log the model\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "            print(f\"Logged model with params: {params} | Accuracy: {accuracy}, ROC AUC: {roc_auc}, F1 Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "dataset_path = 'aclImdb'\n",
    "\n",
    "# Download and extract the dataset\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(\"Downloading and extracting dataset...\")\n",
    "    download_and_extract_dataset(dataset_url, '.')\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IMDb Sentiment Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load positive and negative reviews into Spark DataFrames\n",
    "pos_train_df = load_reviews(spark, os.path.join(dataset_path, \"train/pos\"), 1)\n",
    "neg_train_df = load_reviews(spark, os.path.join(dataset_path, \"train/neg\"), 0)\n",
    "\n",
    "train_df = pos_train_df.union(neg_train_df)\n",
    "\n",
    "train_df = train_df.orderBy(rand())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "X,y = preprocess_data(train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_log_models(logistic_params,X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
